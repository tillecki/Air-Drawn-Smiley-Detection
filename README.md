The Air-Drawn Smiley Detector project explores the use of transformed 3D-accelerometer data into 2D images to interpret hand-drawn smiley faces in the air. Leveraging an iPhone, we generated a dataset comprising 200 training and 30 test samples. By facilitating data augmentation we were able to virtually increase the dataset size. Two distinct approaches were employed: firstly, a ResNet-based model trained on the accelerometer data achieved an  90% accuracy. Secondly, a transfer learning approach utilizing pre-trained VGG19 and ResNet18 models from ImageNet demonstrated promising results, with ResNet achieving 96% and VGG19 reaching 97%. This is already promising, however, future endeavours are needed to fine tune the models and make better interpretable 3D-to-2D transformations.
