The Air-Drawn Smiley Detector project explores the use of transformed 3D-accelerometer data into 2D images to interpret hand-drawn smiley faces in the air. Leveraging an iPhone, we generated a dataset comprising 200 training and 30 test samples. By facilitating data augmentation we were able to virtually increase the dataset size by a factor of 4. Two distinct approaches were employed: firstly, a ResNet-based model trained on the accelerometer data achieved an 80\% accuracy. Secondly, a transfer learning approach utilizing pre-trained VGG19 and ResNet18 models from ImageNet demonstrated promising results, with ResNet achieving 63\% and VGG19 reaching 83\%. This is already promising, however, future endeavours are needed to fine tune the models and make better interpretable 3D-to-2D transformations.
